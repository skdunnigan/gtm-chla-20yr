---
title: "Prepping Chlorophyll-a data from GTMNERR SWMP"
format: 
  html:
    toc: true
    code-fold: true
    embed-resources: true
execute: 
  warning: false
  message: false
editor_options: 
  chunk_output_type: console
bibliography: references.bib
---

```{r}
#| label: setup
#| include: false
if(!require(here)){ install.packages("here") } ;  library(here) # easy paths
if(!require(dplyr)){ install.packages("dplyr") } ;  library(dplyr) # left_join
if(!require(tidyr)){ install.packages("tidyr") } ;  library(tidyr) # pipe operator %>% 
if(!require(ggplot2)){install.packages("ggplot2")} ; library(ggplot2) # plotting
if(!require(lubridate)){ install.packages("lubridate") } ;  library(tidyr) # pipe operator %>% 
if(!require(janitor)){ install.packages("janitor") } ;  library(janitor) # clean names
if(!require(readxl)){ install.packages("readxl") } ;  library(readxl) # clean names
if(!require(wqtrends)){ 
  options(repos = c(
  tbeptech = 'https://tbep-tech.r-universe.dev',
  CRAN = 'https://cloud.r-project.org'))
  install.packages('wqtrends')} ; library(wqtrends) # models and plots for trends

if(!require(mgcv)){ install.packages("mgcv") } ;  library(mgcv)
if(!require(patchwork)) { install.packages("patchwork") }  ; library(patchwork)

```

# Data

## Collection Methods

Water samples were collected monthly at each of the four water quality monitoring stations at the GTMNERR for nitrite + nitrate, ammonium, total nitrogen, orthophosphate, total phosphorus, chlorophyll a, and total suspended solids using two types of sampling methods (grab and diel), on the same day during an ebb tide. Grab samples were collected from all stations in duplicate using a pole sampler and bucket. Diel samples were only collected at the Pellicer Creek station at 2.5-hour intervals for a 25-hour (lunar day) period using a Teledyne ISCO automatic sampler equipped with an internal ice bath to keep samples cold. Both grab and diel samples were collected from the approximate depth of the sonde sensors, 1 m above the bottom. Samples were filtered in the field whenever feasible; otherwise, they were placed on ice in the dark and filtered immediately upon returning to the laboratory.

Water samples collected between January 1, 2003 and June 30, 2010 were collected and analyzed by University of Florida's (UF) Department of Fisheries and Aquatic Sciences. Samples collected between July 1, 2010 and November 30, 2012 were collected by GTMNERR and analyzed by the UF lab. Samples collected on or after December 1, 2012 were collected by GTMNERR and analyzed by the Florida Department of Environmental Protection's (FDEP) Central Laboratory. Both laboratories were certified through the National Environmental Laboratory Accreditation Program and used the American Public Health Association and Environmental Protection Agency methods for analyses. Method details can be found in annual metadata files at www.nerrsdata.org.

## The data set

Loading the nutrient data file and cleaning it up for analysis. This file is an appended internal file to the GTMNERR that contains data from 2002-2023 with QC flags and codes and all the parameters included. This data is not included in the GitHub repository, but can be requested from the GTMNERR.

```{r}
#| label: load-data
#| echo: false

nms <- names(read_excel(here::here('data',
                                   'All_inclusive_NUT',
                                   'gtmnut2002-2023_QC_zeros-corrected.xlsx'), 
                        n_max = 0)) # pull out all the column names in this file

class <- ifelse(grepl("^F_", nms), "text", "numeric") # read everything with F_ as a character
class2 <- class[-(1:5)] # remove the first five elements of the vector because they are different

NUT <- readxl::read_xlsx(here::here('data',
                                    'All_inclusive_NUT',
                                    'gtmnut2002-2023_QC_zeros-corrected.xlsx'),
                         col_types = c("text", 
                                       "date", 
                                       "numeric", 
                                       "numeric", 
                                       "text", 
                                       class2)) %>% # specify how to read in these columns
  janitor::clean_names()

# clean environment
rm(nms, class, class2)

glimpse(NUT)

```

# Data preparation

## Selecting and filtering QAQC flags and codes

We are interested in examining trends in chlorophyll *a* (chl-a) for complete years across a 20 year period. Therefore, we are only selecting the chl-a data from 2003-2022 at all stations. This dataset includes both grab and diel collection results, so the diel data will be filtered out as well.

```{r}
#| label: chla-dat

chla <- NUT %>% 
          filter(!is.na(rep)) %>% # remove "S" reps in dataset
          select(station_code, 2:5, chla_n, f_chla_n) %>% # keep only chla data
          rename(datetimestamp = date_time_stamp) %>% # clean name
          mutate(date = lubridate::date(datetimestamp)) %>% # create date variable from POSIXct
          filter(monitoring_program == 1) %>% # keep only grab samples (remove DIEL)
          select(-monitoring_program, -rep) # remove columns once done

glimpse(chla)

```

The F_Record column contains [QAQC codes](https://cdmo.baruch.sc.edu/data/qaqc.cfm) that apply to the entire record. We want to look at what all are included in this dataset.

```{r}
# examine f_record values for further filtering
unique(chla$f_record)

```

All of those codes are okay, none of the records require further filtering. Next will be to look at the flags and codes for the chl-a results:

```{r}
# examine f_chla values for further filtering
unique(chla$f_chla_n)

```

This data contains several records that will not be good to keep for the analysis. Examining the suspect data ("\<1\>") it seems a lot of that data contains CSM codes and metadata documentation that says these reference "Laboratory indicated that the values reported are the mean of two or more determinations" and values were estimated. For now, all rejected and suspect data will be removed for analysis. I also removed the data flagged with "\<-4\>" which means "below sensor limit". This means this data was below the minimum detection limit from lab.

### Nominal base MDL

There are also quite a few very low values that occur early on in the timeseries:

```{r}
head(chla %>% arrange(chla_n))
```

These values fall below the nominal base minimum detection limit (MDL) for chl-a used in the last 10 years of the record which is 0.55($\mu$g/L). Therefore, any values less than this MDL will be set to that MDL. This will apply to these records:

```{r}
chla %>% filter(chla_n < 0.55)
```

This will replace all these values with the nominal base MDL of 0.55($\mu$g/L).

```{r}
chla$chla_n[chla$chla_n < 0.55] <- 0.55 # replace all values below nominal base mdl of 0.55 with the base mdl 0.55
```

## Formatting for analysis

So, filtering the data to remove unusable QC data, then averaging the duplicates for a monthly average chl-a at each site (`value`), and then adding information to match the format used for the `wqtrends` [workflow](https://tbep-tech.github.io/wqtrends/articles/introduction.html) [@beck2022]. This means adding a few columns such as `doy` (day of year), `cont_year` (date in decimal time), `yr` (year), `mo` (month as character label), and `param` which is probably not necessary, but since it was included in the vignette, I added a column to make it clear this is chl-a data. This is also where I filtered to remove 2002 and 2023 data since they are not of interest for this trend analysis.

```{r}
# prep data for GAM

chla_dat <- chla %>% 
              filter(!grepl(c("<-3>"), f_chla_n) & 
                       !grepl(c("<1>"), f_chla_n) &
                        !grepl(c("<-4>"), f_chla_n)) %>% # remove rejected and suspect data
              select(-f_record, -f_chla_n) %>% # remove qc columns after filtering
              group_by(station_code, date) %>% # group by station and date to average duplicates
              summarise(value = mean(chla_n, na.rm = T)) %>% # avg duplicates
              ungroup() %>% 
              mutate(doy = lubridate::yday(date), # day of the year
                     cont_year = lubridate::decimal_date(date), # date in decimal time
                     yr = lubridate::year(date), # year
                     mo = lubridate::month(date, label = TRUE), # month
                     param = "chla") %>% # add param name
              rename(station = station_code) %>% # clean variable name
              filter(yr > 2002 & yr < 2023) # only keep data from 2003-2022

head(chla_dat)
```

## Values

I want to examine the data available for each station and see where data may be missing.

```{r}
chla_dat %>% 
  filter(station == "gtmpinut") %>%
  select(value, yr, mo) %>% 
  pivot_wider(names_from = mo, values_from = value, id_cols = yr)
```

gtmpinut data missing for 2005-01, 06-2009, 02-2010, 08-2015, 08-2020.

```{r}
chla_dat %>% 
  filter(station == "gtmssnut") %>%
  select(value, yr, mo) %>% 
  pivot_wider(names_from = mo, values_from = value, id_cols = yr)
```

gtmssnut data missing for 2005-01, 06-2009, 02-2010, 09-2019, 08-2020, 07-2022

```{r}
chla_dat %>% 
  filter(station == "gtmfmnut") %>%
  select(value, yr, mo) %>% 
  pivot_wider(names_from = mo, values_from = value, id_cols = yr)
```

gtmfmnut needs NAs placed for 2005-01, 06-2009, 02-2010, 11-2015

```{r}
chla_dat %>% 
  filter(station == "gtmpcnut") %>%
  select(value, yr, mo) %>% 
  pivot_wider(names_from = mo, values_from = value, id_cols = yr)
```

gtmpcnut needs NAs placed for 2003-01, 2005-01, 2009-09, 2010-02, 2013-07, 2015-08, 2015-09, 2018-11, 2020-07, 2022-04, 2022-07, 2022-08

*this may be why the data is not normally distributed...too much is missing. May need to see if I can use the ISCO data to fill in the missing information.*

## Incorporating ISCO data

I'll need to find ISCO data for at least 2013-07,2015-08, 2015-09 (not usable), 2018-11, 2022-04, 2022-07, 2022-08 for ISCO dates. Use the closest value to a similar time for the grab collections

The Pellicer Creek data requires:

Date of GRAB, Date of ISCO, HT time on Grab and HT time on ISCO I'll plan to try and use the closest value to a similar time for the grab collections

```{r}
pc2013 <- NUT %>% 
            filter(!is.na(rep)) %>% # remove "S" reps in dataset
            select(station_code, 2:5, chla_n, f_chla_n) %>% # keep only chla data
            rename(datetimestamp = date_time_stamp) %>% # clean name
            mutate(date = lubridate::date(datetimestamp)) %>%  # create date variable from POSIXct
            filter(station_code == "gtmpcnut") %>% 
            filter(grepl("2013-07-18 07:00", datetimestamp) | 
                     grepl("2013-07-18 09:30", datetimestamp)) %>% 
            group_by(station_code, date) %>% 
            summarize(value = mean(chla_n)) %>% 
            ungroup()

### come back to 08 2015 given the high values
# pc2015 <- NUT %>% 
#             filter(!is.na(rep)) %>% # remove "S" reps in dataset
#             select(station_code, 2:5, chla_n, f_chla_n) %>% # keep only chla data
#             rename(datetimestamp = date_time_stamp) %>% # clean name
#             mutate(date = lubridate::date(datetimestamp)) %>%  # create date variable from POSIXct
#             filter(station_code == "gtmpcnut") %>% 
#             filter(grepl("2015-08", datetimestamp)) 

pc2018 <- NUT %>% 
            filter(!is.na(rep)) %>% # remove "S" reps in dataset
            select(station_code, 2:5, chla_n, f_chla_n) %>% # keep only chla data
            rename(datetimestamp = date_time_stamp) %>% # clean name
            mutate(date = lubridate::date(datetimestamp)) %>%  # create date variable from POSIXct
            filter(station_code == "gtmpcnut") %>% 
            filter(grepl("2018-11-06 10:30", datetimestamp)) %>% 
            select(station_code, date, chla_n) %>% 
  rename(value = chla_n)

# in 2022 they were not collected on the same day as the grab samples.
# cannot use 2022-07
pc2022 <- NUT %>% 
            filter(!is.na(rep)) %>% # remove "S" reps in dataset
            select(station_code, 2:5, chla_n, f_chla_n) %>% # keep only chla data
            rename(datetimestamp = date_time_stamp) %>% # clean name
            mutate(date = lubridate::date(datetimestamp)) %>%  # create date variable from POSIXct
            filter(station_code == "gtmpcnut") %>% 
            filter(grepl("2022-04-18 11:00", datetimestamp) |
                     grepl("2022-08-22 09:00", datetimestamp)) %>% 
            select(station_code, date, chla_n) %>% 
            rename(value = chla_n)

pcISCO <- bind_rows(pc2013, pc2018, pc2022) %>% 
  mutate(doy = lubridate::yday(date), # day of the year
                     cont_year = lubridate::decimal_date(date), # date in decimal time
                     yr = lubridate::year(date), # year
                     mo = lubridate::month(date, label = TRUE), # month
                     param = "chla") %>% # add param name
              rename(station = station_code)

rm(pc2013, pc2018, pc2022)

```

Add the few values from PC ISCO data into the complete dataset

```{r}
chla_dat <- bind_rows(chla_dat, pcISCO)
```

## Preparing the timeseries files

In order to decompose the timeseries, there cannot be any gaps in the data. I need to identify gaps and fill them with NAs

